---
title: "fpp3 hyndman"
subtitle: "Hyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on February 9 2021"
output: 
  html_document: 
    df_print: default
    number_sections: yes
    toc: yes
---

*Tell us what the future holds, so we may know that you are gods.
(Isaiah 41:23)*



```{r setup, include=FALSE, }
library(fpp3)
library(tidyverse)
library(knitr)
library(USgas)
library(readxl)
library(Rfast)
library(GGally)
library(slider)
library(lubridate)
library(glue)
library(broom)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(autodep = TRUE)
```


# Ch1: Getting Started
------------------------------------------------------------

Basic premise of any forecasting:
I need to know what the future holds w/ a given degree of confidence;
I therefore apply the selected model to give a range within which the forecast should lie

*   Start with GRAPHICS
*   Then DECOMPOSE into groups to see patterns
*   FEATURES can be extracted from the data
*   SELECT model and apply model for forecast

Hyndman breaks down forecasting into 3 tasks: Forecasting, Goals and Planning
He also outlines there are Short-term, Medium-term and Long-term forecasts

Quantitative forecasting (as opposed to Qualitative) requires numerical information about the past & a reasonable assumption about autocorrelation

There are 3 general types of models presented

*   Explanatory model -> Y = f(fundamental predictors, $\epsilon$)
*   Time-series model -> Y = f($Y_t$, $Y_{t-1}$, $Y_{t-2}$, $\epsilon$)
*   Mixed model -> Y = f($Y_t$, fundamental predictors, $\epsilon$)

Exercises:

1.  
    * Case 3 possible predictors : previous appraisal value, average vehicle depreciation by model and year, data on possible malfunction, replaced parts, accidents on the vehicles (will affect re-sale value), on demand-side - need to estimate demand for vehicles, and previous values as well
    * Case 4 possible predictors : shocks, factors affecting demand for 3 passenger classes, competition, major events that lead to travel
    
2.  Five steps of forecasting: too verbose to answer on here

Looking forward to `code`

# Ch2: Time series graphics
------------------------------------------------------------

### `tsibble` objects

```{r}
y <- tsibble(
  Year = 2015:2019,
  Observation = c(123, 39, 78, 52, 110),
  index = Year)
head(y)
#index can be specified other than Year, it can be month or day
```

Here is a table for how to use time class functions

Frequency  | Functions
------------- | -------------
Annual  | `start:end`
Quarterly  | `yearquarter()`
Monthly  | `yearmonth()`
Weekly  | `yearweek()`
Daily  | `as_date(), ymd()`
Sub-daily  | `as_datetime()`

```{r}
#this dataset is posted at 4yr intervals
str(olympic_running)
PBS %>%
  filter(ATC2 == "A10") %>%
  select(Month, Concession, Type, Cost) %>%
  summarise(TotalC = sum(Cost)) %>%
  mutate(Cost = TotalC / 1e6) -> a10
#converting a csv into tsibble
prison <- read_csv("https://OTexts.com/fpp3/extrafiles/prison_population.csv")
prison <- prison %>% 
  mutate(Quarter = yearquarter(Date)) %>% 
  #select(-Date) %>% 
  as_tsibble(
    key = c(State, Gender, Legal, Indigenous),
    index = Quarter)
#contains 8 X 2 X 2 X 2 = 64 time series
#64 X 48 Observations for quarters = 3072 rows
prison
```

### Time plots

plot t vs observed y

```{r}
#very messy ts data
mel_syd_economy <- ansett %>% 
  filter(Airports == "MEL-SYD", Class == "Economy")
autoplot(mel_syd_economy, Passengers) +
  labs(title = "Ansett economy class passengers",
       subtitle = "Melbourne-Sydney")
#ts with seasonality, strong trend
autoplot(a10, Cost)
  labs(y = "$million", title = "Antidiabetic drug sales")
```

Hyndman delimits 3 types of ts patterns:
*   Trend - think of it as a greedy line
*   Seasonal - a change occurs at fixed intervals (NOT recession!!)
*   Cyclical - think recession, regime change for financial data

### Seasonal plots

Plot data grouped by seasons

```{r}
#this chart should make clear that january sales are strong in all years
a10 %>% 
  gg_season(Cost, label = "both") +
  labs(y = "$ million",
       title = "Seasonal plot : antidiabetic drug sales")
#the following plot is garbage for now 
#by day
vic_elec %>% gg_season(Demand, period = "day") +
  theme(legend.position = "none")
#by week
vic_elec %>% gg_season(Demand, period = "week") +
  theme(legend.position = "none")
#by year
vic_elec %>% gg_season(Demand, period = "year") +
  theme(legend.position = "right")
```

The strength of `gg_season` plots is that it can visualise possible features of the data

*   Note that the original data `vic_elec` is intact and we have made heavy lifting possible

### Seasonal subseries plots

The main difference here compared to seasonal plots is that the data for each season are presented in split windows

```{r}
#note the blue line shows the mean for the given window
a10 %>% 
  gg_subseries(Cost) +
  labs(
    y = "$ million",
    title = "Seasonal subseries plot: antidiabetic drug sales")
```

Hyndman is Australian, so we will see an example of australian vacation data

```{r}
holidays <- tourism %>% 
  filter(Purpose == "Holiday") %>% 
  group_by((State)) %>% 
  summarise(Trips = sum(Trips))
holidays
#this plot shows strong but differing seasonality in btw groups
autoplot(holidays, Trips) +
  labs(y = "Trips, in thousands",
       title = "Australian domestic holiday nights")
gg_season(holidays, Trips) +
  labs(y = "Trips, in thousands",
       title = "Australian domestic holiday nights")
#the South states are strongest in Q4, while the North states are strongest in Q3
#Western Australia tourism has also jumped recently
holidays %>% gg_subseries(Trips) +
  labs(y = "Trips, in thousands",
       title = "Australian domestic holiday nights")
```

### Scatterplots

Visualising X-Y relationships

```{r}
#temperature levels and Demand levels covary strongly, it seems
vic_elec %>% 
  filter(year(Time) == 2014) %>% 
  autoplot(Demand) +
    labs(
      y = "Demand (gW)",
      title = "Half-hourly electricity demand: Victoria, Australia")
vic_elec %>%
  filter(year(Time) == 2014) %>%
  autoplot(Temperature) +
  labs(
    y = "Temperature (degrees Celsius)",
    title = "Half-hourly temperatures: Melbourne, Australia")
vic_elec %>% 
  filter(year(Time) == 2014) %>% 
  ggplot(aes(x = Temperature, y = Demand)) +
  geom_point() +
    labs(y = "Demand (gW)", x = "Temperature (in degrees Celsius)")
```

Correlation: be aware that this captures the linear r/ship btw X and Y. That is, the temp = Demand r/ship gives `r` = 0.28, which undervalues the non-linear relationship

Next, Hyman wants to plot correlations between tourism by state

```{r}
#facet_wrap is way better than facet_grid
visitors <- tourism %>% 
  group_by(State) %>% 
  summarise(Trips = sum(Trips))
visitors %>% 
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_wrap(vars(State), scales = "free_y") +
    labs(y = "Number of visitor nights per quarter (in Millions)")
visitors %>%
  pivot_wider(values_from = Trips, names_from = State) %>%   
  GGally::ggpairs(columns = 2:9)
```

### Lag plots

```{r}
recent_production <- aus_production %>% 
  filter(year(Quarter) >= 2000)
recent_production %>% gg_lag(Beer, geom = "point")
#this visualization makes clear that at lag = 4 and lag = 8, r is high
#otw r is low at lag = 2 and lag = 6 since the quarters rolls over to the previous year
recent_production %>% gg_lag(Beer, geom = "path")
```

### Autocorrelation

We have arrived!

The ACF function
$$
\def\ybar{\overline{y}}
r_{k} = \frac{\sum_{t=k+1}^{T} (y_t-\ybar)(y_{t-k} - \ybar)}{\sum_{t=1}^{T} (y_t - \ybar)^2}
$$
This measures at lag k, how much the series regresses upon itself, linearly

```{r}
#correlogram
recent_production %>% ACF(Beer, lag_max = 9)
acf(recent_production$Beer, type = "correlation", lag.max = 9, plot = TRUE)
#much seasonal, little trend
recent_production %>% 
  ACF(Beer) %>% 
  autoplot()
#both the trend and seasonality
a10 %>% 
  ACF(Cost, lag_max = 48) %>% 
  autoplot()
```

### White noise

```{r}
set.seed(30)
y <- tsibble(sample = 1:50, wn = rnorm(50), index = sample)
y %>% autoplot(wn) + labs(title = "White noise")
y %>% 
  ACF(wn) %>% 
  autoplot()
```

### Exercises

```{r, include= FALSE}
#Exercises
#2 GAFA
goog_max <- gafa_stock %>%
  mutate(max_Close = max(Close)) %>% 
  filter(Symbol == "GOOG", Close == max_Close)
#3 Tute 1
# tute1 <- read_csv("/Users/Jacques/Downloads/tute1.csv")
# myts <- tute1 %>%
#   mutate(Quarter = yearmonth(Quarter)) %>%
#   as_tsibble(index = Quarter)
# myts %>%
#   pivot_longer(-Quarter) %>%
#   ggplot(aes(x = Quarter, y = value, colour = name)) +
#   geom_line() +
#   facet_grid(name ~ ., scales = "free_y")
#4 USgas
us_total %>% 
  tsibble(
    key = state,
    index = year)
new_england <- us_total %>% 
  group_by(state) %>% 
  filter(
    state == "Maine" | 
    state == "Vermont" | 
    state == "New Hampshire" | 
    state == "Connecticut" |  
    state == "Massachussets" | 
    state == "Rhode Island")
         
new_england %>%         
  ggplot(aes(x = year, y = y)) +
  geom_line() +
  facet_wrap(vars(state), scales = "free_y") +
    labs(title = "Annual Nat Gas consumptions by state for New England")
str(us_total)
#5 Tourism
# tourism_xl <- read_xlsx("/Users/Jacques/Downloads/tourism.xlsx")
# #view(tourism_xl)
# tourism_xl <- tourism_xl %>% 
#   mutate(Quarter = yearquarter(Quarter)) %>% 
#   as_tsibble(
#     key = c(Region, State, Purpose),
#     index = Quarter)
#tourism == tourism_xl
#
# tourism_ft <- tourism_xl %>%
#   group_by(Region, Purpose) %>% 
#   slice_max(Trips, n = 1)
# row_ft <- which.max(tourism_ft$Trips)
# max_pair <- c(tourism_ft[row_ft, 2], tourism_ft[row_ft, 4])
# max_pair
#6 Time plots
autoplot(aus_production, Bricks)
autoplot(pelt, Lynx)
autoplot(gafa_stock, Close)
autoplot(vic_elec, Demand) +
  labs(y="Demand, in gW",
       x ="Date",
       title = "Electricity demand for Victoria, Australia")
#7 aus_arrivals
aus_arrivals
aus_arrivals_ft <- aus_arrivals %>% 
  group_by(Origin)
aus_arrivals_ft %>% 
  autoplot(Arrivals)
aus_arrivals_ft %>% 
  gg_season(Arrivals)
aus_arrivals_ft %>% 
  gg_subseries(Arrivals)
#I identify odd points 
#at Q3, US around 1990 the double spike
#at Q2, Q3, Q4 of Japan around 2004 the dip is off bc we dont see it in other countries
#Q2 and Q3 data for UK is near flat
#8 aus_retail
aus_retail
set.seed(12345678)
myseries <- aus_retail %>%
  filter(`Series ID` == sample(aus_retail$`Series ID`,1))
#strong trend, seasonality in December month and slightly in summer
#1996-1997 there is a massive spike that looks unusual
myseries %>% 
  autoplot(Turnover)
myseries %>% 
  gg_season(Turnover)
myseries %>% 
  gg_subseries(Turnover)
myseries %>% 
  gg_lag(Turnover, lags = 1:12)
us_employment
total_private <- us_employment %>%
  filter(Title == "Total Private")
 
total_private %>% 
  autoplot(Employed)
#thick lines usually means NBER recessions
#really high AR
total_private %>% 
  gg_season(Employed)
total_private %>% 
  gg_subseries(Employed)
acf(total_private$Employed, plot = TRUE)
#10
#3 == D
#2 == A
#1 == B
#4 == C
#11 aus_livestock
 
#12 Goog daily price   
dgoog <- gafa_stock %>%
  filter(Symbol == "GOOG", year(Date) >= 2018) %>%
  mutate(trading_day = row_number()) %>%
  update_tsibble(index = trading_day, regular = TRUE) %>%
  mutate(diff = difference(Close))
```

# Ch3: Time series decomposition
------------------------------------------------------------

Subgrouping our data into "facets" is very useful for forecasting
Three components: A trend-cycle comp., a seasonal component and a remainder (noise, innovation)

### Transformations and adjustments

We can make adjustments based on population, calendar spreads (for ex. different # of days in montly data), inflation

```{r}
#population
global_economy %>%
  filter(Country == "Australia") %>%
  autoplot(GDP/Population) + labs(y = "GDP per capita ($US)")
print_retail <- aus_retail %>%
  filter(Industry == "Newspaper and book retailing") %>%
  group_by(Industry) %>%
  index_by(Year = year(Month)) %>%
  summarise(Turnover = sum(Turnover))
aus_economy <- global_economy %>%
  filter(Code == "AUS")
print_retail %>%
  left_join(aus_economy, by = "Year") %>%
  mutate(Adjusted_turnover = Turnover / CPI * 100) %>%
  pivot_longer(c(Turnover, Adjusted_turnover), 
               values_to = "Turnover") %>%
  ggplot(aes(x = Year, y = Turnover)) +
  geom_line() +
  facet_grid(name ~ ., scales = "free_y") +
  labs(
    y = "$A", 
    title = "Turnover for the Australian print media industry")
```

Mathematical tranformations can be made to the observational data
For example, we can use the power transformations $w_t = y_t^p, p\in {N}$

The Box-Cox transformations are a special kind of power trasnformations
$$
w_t  =
    \begin{cases}
      \log(y_t) && if \lambda=0;  \\
      \text{sign}(y_t)(|y_t|^\lambda-1)/\lambda && \text{otherwise}.
    \end{cases}
$$
A `guerrero` feature automatically selects a $\lambda$ value for you.

```{r}
lambda <- aus_production %>%
  features(Gas, features = guerrero) %>%
  pull(lambda_guerrero)
aus_production %>% autoplot(box_cox(Gas, lambda))
  
```

### Time series components

$y_{t} = S_{t} + T_{t} + R_t$
$y_t$ is the data, $S_t$ is the seasonal comp, $T_t$ is the trend-cycle comp and $R_t$ is the noise

```{r}
us_retail_employment <- us_employment %>%
  filter(year(Month) >= 1990, Title == "Retail Trade") %>%
  select(-Series_ID)
autoplot(us_retail_employment, Employed) +
  labs(
    y = "Persons (thousands)", 
    title = "Total employment in US retail"
  )
dcmp <- us_retail_employment %>%
  model(STL(Employed))
autoplot(us_retail_employment, Employed, color = "gray") +
  autolayer(components(dcmp), trend, color = "red") +
  labs(
    y = "Persons (thousands)", 
    title = "Total employment in US retail"
  )
components(dcmp) %>% autoplot()
autoplot(us_retail_employment, Employed, color = "gray") +
  autolayer(components(dcmp), season_adjust, color = "blue") +
  labs(
    y = "Persons (thousands)", 
    title = "Total employment in US retail"
  )
```

### Moving averages

The m-MA is written as
$$
\begin{equation}
  \hat{T}_{t} = \frac{1}{m} \sum_{j=-k}^k y_{t+j}
\end{equation}
$$

```{r}
global_economy %>%
  filter(Country == "Australia") %>%
  autoplot(Exports) +
  labs(y = "% of GDP", title = "Total Australian exports")
aus_exports <- global_economy %>%
  filter(Country == "Australia") %>%
  mutate(
    `5-MA` = slider::slide_dbl(Exports, mean, 
                .before = 2, .after = 2, .complete = TRUE)
  )
autoplot(aus_exports, Exports) +
  autolayer(aus_exports, `5-MA`, color = "red") +
  labs(
    y = "Exports (% of GDP)", 
    title = "Total Australian exports"
  ) +
  guides(colour = guide_legend(title = "series"))
#MA of MA
beer <- aus_production %>%
  filter(year(Quarter) >= 1992) %>%
  select(Quarter, Beer)
beer_ma <- beer %>%
  mutate(
    `4-MA` = slider::slide_dbl(Beer, mean, 
                .before = 1, .after = 2, .complete = TRUE),
    `2x4-MA` = slider::slide_dbl(`4-MA`, mean, 
                .before = 1, .after = 0, .complete = TRUE)
  )
us_retail_employment_ma <- us_retail_employment %>%
  mutate(
    `12-MA` = slider::slide_dbl(Employed, mean, 
                .before = 5, .after = 6, .complete = TRUE),
    `2x12-MA` = slider::slide_dbl(`12-MA`, mean, 
                .before = 1, .after = 0, .complete = TRUE)
  )
autoplot(us_retail_employment_ma, Employed, color = "gray") +
  autolayer(us_retail_employment_ma, vars(`2x12-MA`), 
            color = "red") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")
```

An important note is that for even seasons (ex. quarter, months) is it highly recommended to use a 2Xm-MA for m seasons.
For odd seasons, or other uses, use the simple m-MA which is already centered

Moving averages can also be computed with a weighted window which smooths out the endpoint effect
an example is the following window: $\left[\frac{1}{8},\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{8}\right]$

### X11 decomp

This method was developed by BLS and StatsCan. It starts with a classical decomp, and lets the seasonal comp drift. It also allows for some irregulary spaced calendars. It tends to be way more robust than the classical decomp

```{r}
#this code doesn't work!!
# x11_dcmp <- us_retail_employment %>%
#   model(x11 = feasts:::X11(Employed, type = "additive")) %>%
#   components()
# autoplot(x11_dcmp) +
#   labs(title =
#     "Additive X11 decomposition of US retail employment")
#
# x11_dcmp %>%
# ggplot(aes(x = Month)) +
# geom_line(aes(y = Employed, colour = "Data")) +
# geom_line(aes(y = season_adjust,
#               colour = "Seasonally Adjusted")) +
# geom_line(aes(y = trend, colour = "Trend")) +
# labs(y = "Persons (thousands)",
#      title = "Total employment in US retail") +
# scale_colour_manual(
#   values = c("gray", "blue", "red"),
#   breaks = c("Data", "Seasonally Adjusted", "Trend")
# )
# # x11_dcmp %>%
# gg_subseries(seasonal)
```

### SEATS decomp

SEATS = Seasonal Extraction of ARIMA Time Series
widely used at government agencies

This only works with monthly and qtrly data

```{r}
# tourism %>%
#   group_by(Purpose) %>%
#   summarise(Trips = sum(Trips)) %>%
#   model(STL(Trips ~ season(window = 5))) %>%
#   components()
# seats_dcmp <- us_retail_employment %>%
#   model(seats = feasts:::SEATS(Employed)) %>%
#   components()
# autoplot(seats_dcmp) +
#   labs(title =
#     "SEATS decomposition of total US retail employment")
#
# seats_dcmp <- us_retail_employment %>%
# model(seats = feasts:::SEATS(Employed)) %>%
#   components()
```

### STL decomp

It is the recommended method
If $\lambda = 1$ then it is set to additive decompostion while a $\lambda = 0$ parameter yield multiplicative decomp

```{r}
#trend window is the is the # of consecutive observations to be used for T_t
#using a longer trend window creates leakages from regim changes into the remainder
#default is set to season(window = 13) and trend(window = 21)
us_retail_employment %>%
  model(
    STL(Employed ~ trend(window = 7) + 
              season(window = "periodic"),
    robust = TRUE)) %>%
  components() %>%
  autoplot()
```

### Exercises

```{r, include=FALSE}
```

# Ch4: Time series features
------------------------------------------------------------

### Some statistics

```{r}
tourism %>% 
  features(Trips, list(mean = mean)) %>% 
  arrange(mean)
tourism %>% 
  features(Trips, list(quantile, mean = mean)) %>% 
  arrange(mean)
  
  
```

### ACF features

He includes differencing here as well as some more pointy stats related to acf
The `feat_acf()` function wraps many of these features into one call.
*   acf1 means the first autocorr coefficient
*   acf10 stands for the sum of the first ten autocorr coefficients
*   diff1_acfn takes the above measurements on a diff(x) series

```{r}
tourism %>% features(Trips, feat_acf)
```

### STL features

Recall we have decomposed the ts $y_t = S_t + T_t + R_t$

For strong trend data(or seasonality), the expression $\frac{\text{Var}(R_t)}{\text{Var}(T_t+R_t)}$ will be small (<1) and thus $F_t$ will approach 1.
$$
F_T = \max\left(0, 1 - \frac{\text{Var}(R_t)}{\text{Var}(T_t+R_t)}\right)
$$
Note the above expression for trend is similarly used to measure seasonality. Strong seasonality will give a number closer to 1

```{r}
tourism %>%
  features(Trips, feat_stl) 
tourism %>%
  features(Trips, feat_stl) %>% 
  arrange(trend_strength)
tourism %>%
  features(Trips, feat_stl) %>%
  ggplot(aes(x = trend_strength, y = seasonal_strength_year, 
             col = Purpose)) +
  geom_point() +
  facet_wrap(vars(State))
#this code shows the most seasonal series
tourism %>%
  features(Trips, feat_stl) %>%
  filter(
    seasonal_strength_year == max(seasonal_strength_year)
  ) %>%
  left_join(tourism, by = c("State", "Region", "Purpose")) %>%
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_grid(vars(State, Region, Purpose))
```

### Exploring autralian tourism data

```{r}
#extracts all features from feasts (48)
tourism_feats <- tourism %>% 
  features(Trips, feature_set(pkgs = "feasts"))
tourism_feats
#using glue pkg
#be aware of the syntax
#we notice that the 3 seasonal measures are positively correlated
#we also notice that the most seasonal series is holidays (as expected)
tourism_feats %>% 
  select_at(vars(contains("season"), Purpose)) %>% 
  mutate(
    seasonal_peak_year = glue("Q{seasonal_peak_year+1}"),
    seasonal_trough_year = glue("Q{seasonal_trough_year+1}"),
  ) %>% 
  GGally::ggpairs(mapping = aes(colour = Purpose))
#using broom pkg
#we can futher explore the 48 features by using pca. pca groups features together and so reduces dimensionality. 
#note that pca1 explains most variations of the date &
#pca2 is meant to be uncorrelated to pca1
#the plot tells us that PC2 delineates holiday series
pc_dcmp <- tourism_feats %>% 
  select(-State, -Region, -Purpose) %>%
  prcomp(scale = TRUE) %>% 
  augment(tourism_feats)
pc_dcmp %>% 
  ggplot(aes(x= .fittedPC1, y = .fittedPC2, col = Purpose)) +
  geom_point() +
  theme(aspect.ratio = 1)
#drill to the anomalies in pca
outliers <- pc_dcmp %>%
  filter(.fittedPC1 > 10) %>%
  select(Region, State, Purpose, .fittedPC1, .fittedPC2)
outliers %>%
  left_join(tourism, by = c("State", "Region", "Purpose")) %>%
  mutate(
    Series = glue("{State}", "{Region}", "{Purpose}", 
                  .sep = "\n\n")
  ) %>%
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_grid(Series ~ ., scales = "free") +
  labs(title = "Outlying time series in PC space")
```

### Exercises

```{r, include= FALSE}
# #1
# key(PBS)
# pbs_ft <- PBS %>%
#   group_by(Concession, Type, ATC1, ATC2) %>% 
#   summarise(
#     mean_Scripts = mean(Scripts),
#     mean_Cost = mean(Cost),
#     sd_Scripts = sd(Scripts),
#     sd_Cost = sd(Cost)) %>%
#   filter(mean_Cost == max(mean_Cost) |
#          sd_Cost == min(sd_Cost))
# 
# #2 
# 
# tourism_feats_holidays <- tourism_feats %>% 
#   filter(Purpose == "Holiday")
# holidays_Rships <- tourism_feats_holidays %>% 
#   select_at(vars(contains("season"), Purpose)) %>%
#   mutate(
#     seasonal_peak_year = glue("Q{seasonal_peak_year+1}"),
#     seasonal_trough_year = glue("Q{seasonal_trough_year+1}"),
#   ) %>% 
#   GGally::ggpairs()
# holidays_Rships
# 
# #3
# 
# pbs_dcmp <- PBS %>%
#   features(Cost, feature_set(pkgs = "feasts"))
# 
# #drill down
# #this bugs
# outliers_pbs <- pbs_dcmp %>%
#   filter(seasonal_strength_year < quantile(seasonal_strength_year, probs = seq(min(seasonal_strength_year), max(seasonal_strength_year), 0.10)),
#          trend_strength < quantile(trend_strength, probs = seq(min(trend_strength), max(trend_strength), 0.10))) %>%
#   select(Concession, Type, ATC1, ATC2, trend_strength, seasonal_strength_year)
# 
# #drill up
# outliers %>%
#   left_join(PBS, by = c("Concession", "Type", "ATC1", "ATC2")) %>%
#   mutate(
#     Series = glue("{Concession}", "{Type}", "{ATC1}", "{ATC2}", 
#                   .sep = "\n\n")
#   ) %>%
#   ggplot(aes(x = Month, y = Cost)) +
#   geom_line() +
#   facet_grid(Series ~ ., scales = "free") +
#   labs(title = "Outlying time series in PC space")
```

# Ch5: Forecaster's toolbox
------------------------------------------------------------

### Tidy workflow

tidy -> ((visualise -> specify -> estimate -> evaluate -> visualise)) -> forecast

we will be using gdp data

```{r}
#tiny tidy
gdppc <- global_economy %>% 
  mutate(GDP_per_capita = GDP / Population)
#visualise
gdppc %>%
  filter(Country == "Sweden") %>%
  autoplot(GDP_per_capita) +
  labs(y = "$US", title = "GDP per capita for Sweden")
#specify/estimate
fit <- gdppc %>%
  model(trend_model = TSLM(GDP_per_capita ~ trend()))
#skip evaluate
#forecast
fit %>% forecast(h = "3 years")
fit %>%
  forecast(h = "3 years") %>%
  filter(Country == "Sweden") %>%
  autoplot(gdppc) +
  labs(y = "$US", title = "GDP per capita for Sweden")
  
```

### basic forecasting tools

these will only be considered for benchmarking purposes

```{r}
#truncate a few years
bricccsquad <- aus_production %>% 
  filter_index("1970 Q1" ~ "2004 Q4")
#average method
mean_brick <- bricccsquad %>% 
  model(mean_model = MEAN(Bricks))
mean_brick %>% 
  forecast(h = "3 years") %>% 
  autolayer(bricccsquad)
#naive method
bricccsquad %>% 
  model(NAIVE(Bricks))
#Drift method
bricccsquad %>% 
  model(RW(Bricks ~ drift()))
#combining all above methods
# Set training data from 1992 to 2006
train <- aus_production %>%
  filter_index("1992 Q1" ~ "2006 Q4")
# Fit the models
beer_fit <- train %>%
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer)
  )
# Generate forecasts for 14 quarters
beer_fc <- beer_fit %>% forecast(h = 14)
# Plot forecasts against actual values
beer_fc %>%
  autoplot(train, level = NULL) +
  autolayer(
    filter_index(aus_production, "2007 Q1" ~ .),
    color = "black"
  ) +
  labs(
    y = "Megalitres",
    title = "Forecasts for quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))
#try for the bricks
# Set training data from 1992 to 2006
train_briccc <- aus_production %>%
  filter_index("1990 Q1" ~ "2004 Q4")
# Fit the models
brick_fit <- train %>%
  model(
    Mean = MEAN(Bricks),
    `Naïve` = NAIVE(Bricks),
    `Seasonal naïve` = SNAIVE(Bricks))
# Generate forecasts for 14 quarters
brick_fc <- brick_fit %>% 
  forecast(h = 14)
# Plot forecasts against actual values
brick_fc %>%
  autoplot(train, level = NULL) +
  autolayer(
    filter_index(aus_production, "2005 Q1" ~ .),
    color = "black"
  ) +
  labs(
    y = "Bricks",
    title = "Forecasts for quarterly bricks production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))
#he also likes to forecast google stock price 1-mt ahead
# Re-index based on trading days
google_stock <- gafa_stock %>%
  filter(Symbol == "GOOG", year(Date) >= 2015) %>%
  mutate(day = row_number()) %>%
  update_tsibble(index = day, regular = TRUE)
# Filter the year of interest
google_2015 <- google_stock %>% filter(year(Date) == 2015)
# Fit the models
google_fit <- google_2015 %>%
  model(
    Mean = MEAN(Close),
    `Naïve` = NAIVE(Close),
    Drift = NAIVE(Close ~ drift())
  )
# Produce forecasts for the trading days in January 2016
google_jan_2016 <- google_stock %>%
  filter(yearmonth(Date) == yearmonth("2016 Jan"))
google_fc <- google_fit %>%
  forecast(new_data = google_jan_2016)
# Plot the forecasts
google_fc %>%
  autoplot(google_2015, level = NULL) +
  autolayer(google_jan_2016, Close, color = "black") +
  labs(x = "Day", y = "Closing Price (US$)",
       title = "Google stock prices (Jan 2015 - Jan 2016)") +
  guides(colour = guide_legend(title = "Forecast"))
#as we can see this method is not necessarily the strongest
```

### Fitted values and resids

`augment()` function can automatically parse the model for the `.fitted, .resid, .innov` parts of the model

```{r}
augment(beer_fit)
augment(google_fit)
```

### Residual diagnostics

```{r}
google_2015 %>%
  model(NAIVE(Close)) %>%
  gg_tsresiduals()
```

Hyndman introduces several tests -
a portmanteau test is a test on a group of ac's
-notable are the Box-Pierce and Ljung-Box tests (more accurate)
the L-B test computes this: 
$$
Q^* = T(T+2) \sum_{k=1}^\ell (T-k)^{-1}r_k^2
$$
`lag = l`

```{r}
aug <- google_2015 %>%
  model(NAIVE(Close)) %>%
  augment()
#these both show that the residuals are not distinguishable from wn
aug %>% features(.innov, box_pierce, lag = 10, dof = 0)
aug %>% features(.innov, ljung_box, lag = 10, dof = 0)
fit <- google_2015 %>% model(RW(Close ~ drift()))
tidy(fit)
augment(fit) %>% features(.innov, ljung_box, lag=10, dof=1)
```

### Distributional forecasts and prediction intervals

h-step prediction intervals are given as:
$$
\hat{y}_{T+h|T} \pm c \hat\sigma_h
$$
where c=1.96 for a 95% PI
It is normal to see the interval widen with h
We are given the following values for the 4 methods we have examined so far:

*   Mean forecast: $\hat\sigma_h = \hat\sigma\sqrt{1 + 1/T}$
*   Naive forecast: $\hat\sigma_h = \hat\sigma\sqrt{h}$
*   SNaive forecast: $\hat\sigma_h = \hat\sigma\sqrt{k+1}$
*   Drift forecast: $\hat\sigma_h = \hat\sigma\sqrt{h(1+h/T)}$

Thus when $h=1$ and $T$ grows unbounded all these are the same
```{r}
#80p and 95p PI for google stock
google_2015 %>%
  model(NAIVE(Close)) %>%
  forecast(h = 10) %>%
  autoplot(google_2015)
#bootstrapped residuals
fit <- google_2015 %>%
  model(NAIVE(Close))
sim <- fit %>% generate(h = 30, times = 5, bootstrap = TRUE)
sim
#plot the 5 paths
google_2015 %>%
  ggplot(aes(x = day)) +
  geom_line(aes(y = Close)) +
  geom_line(aes(y = .sim, colour = as.factor(.rep)),
    data = sim) +
  labs(title = "Google closing stock price") +
  guides(col = FALSE)
#bootstrap PI
fc <- fit %>% forecast(h = 30, bootstrap = TRUE)
autoplot(fc, google_2015) +
  labs(title = "Google stock closing price")
google_2015 %>% 
  model(NAIVE(Close)) %>% 
  forecast(h = 10, bootstrap = TRUE, times = 1000) %>% 
  hilo()
```

### Forecasting with decomposition

```{r}
us_retail_employment <- us_employment %>%
  filter(year(Month) >= 1990, Title == "Retail Trade")
dcmp <- us_retail_employment %>%
  model(STL(Employed ~ trend(window = 7), robust = TRUE)) %>%
  components() %>%
  select(-.model)
dcmp %>%
  model(NAIVE(season_adjust)) %>%
  forecast() %>%
  autoplot(dcmp) +
  labs(y = "Number of people employed",
       title = "Naïve forecasts of seasonally adjusted data")

fit_dcmp <- us_retail_employment %>%
  model(stlf = decomposition_model(
    STL(Employed ~ trend(window = 7), robust = TRUE),
    NAIVE(season_adjust)
  ))
fit_dcmp %>%
  forecast() %>%
  autoplot(us_retail_employment)+
  labs(y = "Number of people employed",
       title = "Monthly US employment")
```

Skipping sections until 7.5, with some sparse aadditions of code

```{r}
#from 7.1
us_change %>% 
  GGally::ggpairs(columns = 2:6)

us_change %>% 
  model(TSLM(Consumption ~ Income)) %>% 
  report()

us_change %>% 
  model(TSLM(Consumption ~ Income)) %>% 
  report()

#from 7.2
fit.consMR <- us_change %>%
  model(tslm = TSLM(Consumption ~ Income + Production +
                                    Unemployment + Savings))
report(fit.consMR)

augment(fit.consMR) %>%
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = Consumption, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = NULL,
    title = "Percent change in US consumption expenditure"
  ) +
  scale_color_manual(values=c(Data="black",Fitted="red")) +
  guides(colour = guide_legend(title = NULL))

```
# Ch7: TS regression models
------------------------------------------------------------

### Evaluating regression model

The method presented in this book is wildly different to the one presented in modelr pkg. We will re-use the conMR model presented above

```{r}
#gg_tsdisplay is a panel chart that wraps the data for displaying one series' acf 
fit.consMR %>% gg_tsresiduals()

tsibbledata::aus_retail %>%
  filter(
    State == "Victoria",
    Industry == "Cafes, restaurants and catering services"
  ) %>%
  gg_tsdisplay(Turnover, plot_type = "scatter")

augment(fit.consMR) %>% 
  features(.innov, ljung_box, lag = 10, dof = 5)

us_change %>%
  left_join(residuals(fit.consMR), by = "Quarter") %>%
  pivot_longer(Income:Unemployment,
               names_to = "regressor", values_to = "x") %>%
  ggplot(aes(x = x, y = .resid)) +
  geom_point() +
  facet_wrap(. ~ regressor, scales = "free_x") +
  labs(y = "Residuals", x = "")

#it seems hyndman's approach is way more scalable to more models than the one prescribed in r4ds

augment(fit.consMR) %>% 
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() +
  labs(x = "Fitted",
       y = "Rediduals")

```
### Useful predictors

trend, dummies, interventions, other effects
```{r}
#TSLM automatically handles the dummies when season() is added
recent_production <- aus_production %>%
  filter(year(Quarter) >= 1992)
fit_beer <- recent_production %>%
  model(TSLM(Beer ~ trend() + season()))
report(fit_beer)
#what the report says is : Q2 produces 34.7megaL less than in Q1, Q4 produces 72megaL more than Q1

augment(fit_beer) %>%
  ggplot(aes(x = Beer, y = .fitted,
             colour = factor(quarter(Quarter)))) +
  geom_point() +
  labs(y = "Fitted", x = "Actual values",
       title = "Quarterly beer production") +
  geom_abline(intercept = 0, slope = 1) +
  guides(colour = guide_legend(title = "Quarter"))
```
### Selecting predictors

This section deals with the alphabet of AIC, BIC, etc which are measures of efficiency btw models

*   Adj $R^2$: penalizes overfitting by adding more quantity of predictors
*   Cross-validation (CV): comes from models fitted when leaving out observations. these yield errors and thus MSE. we minimize the CV
*   Akaike Info Criterion (AIC): AIC measures the error from the model and adds penalty for the nb of predictors
*   Corrected Akaike Info Criterion (AIC): AIC corrected for small sample models
*   Bayesian Info Criterion (BIC): AIC but with a larger penalty

For large datasets, these indicators will yield more or less the same results

```{r}
glance(fit.consMR) %>% 
  select(adj_r_squared, CV, AIC, AICc, BIC)

fit.consMR_2 <- us_change %>%
  model(
    tslm1 = TSLM(Consumption ~ Income + Production + Unemployment + Savings),
    tslm2 = TSLM(Consumption ~ Income + Production + Unemployment),
    tslm3 = TSLM(Consumption ~ Income + Production + Savings),
    tslm4 = TSLM(Consumption ~ Income + Savings + Unemployment),
    tslm5 = TSLM(Consumption ~ Income + Savings),
    tslm6 = TSLM(Consumption ~ Savings + Production + Unemployment),
    tslm7 = TSLM(Consumption ~ Income + Unemployment),
    tslm8 = TSLM(Consumption ~ Income + Production)
    )

report(fit.consMR_2)

```

```{r}
#We are stepping aside for some stepwise regression
#The basic idea is to start with the most general model and work backward form there
#https://www.guru99.com/r-simple-multiple-linear-regression.html#8

df <- mtcars %>% 
mutate(cyl = factor(cyl),
    vs = factor(vs),
    am = factor(am),
    gear = factor(gear),
    carb = factor(carb))

df <- mtcars %>% 
  select(-c(am, vs, cyl, gear, carb))
GGally::ggscatmat(df)

require(olsrr)
model <- mpg ~.
fit <- lm(model, df)
test <- ols_step_all_possible(fit)
plot(test)

steps <- ols_step_both_p(fit, details = TRUE)
steps
```

# Ch8: Exponential smoothing
------------------------------------------------------------

### Simple exponential smoothing (SES)

`model()` calls this `ETS(A,N,N)`

The SES method gives a balanced approach between the Naive method (differencing) and the simple average method (draw a horizontal line through the past)
It puts weights determined by $\alpha$ unto the past observations to generate a one-step ahead forecast
*   Note that small values of $\alpha$ puts more weight on the past while large values of $\alpha$ weight the recent past heavily

$$
\begin{equation}
  \hat{y}_{T+1|T} = \alpha y_T + \alpha(1-\alpha) y_{T-1} + \alpha(1-\alpha)^2 y_{T-2}+ \cdots,   \tag{8.1}
\end{equation}
\text{where 0} \leq \alpha \leq 1
$$
This takes on many forms. One form that comes from comparing all T equations gives the following formula, for $l_0$ is the time 1 fitted values. It shrinks to zero for large T.

$$
\begin{align*}
  \hat{y}_{T+1|T} & =  \sum_{j=0}^{T-1} \alpha(1-\alpha)^j y_{T-j} + (1-\alpha)^T \ell_{0}
\end{align*}
$$

```{r}
algeria_economy <- global_economy %>% 
  filter(Country ==  "Algeria")
algeria_economy %>% 
  autoplot(Exports) +
  labs(y = "% of GDP", title = "Exports: Algeria")

fit_algeria <- algeria_economy %>% 
  model(ETS(Exports ~ error("A") + trend("N") + season("N") ))

fc <- fit_algeria %>% 
  forecast(h=5)

#not that great but OK
fc %>% 
  autoplot(algeria_economy) +
  geom_line(aes(y= .fitted), col ="red",
            data = augment(fit_algeria)) +
  labs(y="% GDP", title = "Exports: Algeria") +
  guides(colour = FALSE)

report(fit_algeria)

```
### Methods with trend

`model()` calls this `ETS(A,A,N)`

The linear trend method has a Forecast equation, a Level equation and a Trend equation
*   Forecast at h = level + h*trend
*   Level is the same as for SES but adds the level to the second part
*   Trend is a two-per weighted avg of trends and represents the change in level
*   The smoothing parameter for the level equation is $0 \leq \alpha \leq 1$
*   The smoothing parameter for the trend equation is $0 \leq \beta^* \leq 1$

It seems that the `ETS(Error, Trend, Season)` call sets "A" as an activator

```{r}

fit_aus <- global_economy %>% 
  filter(Country == "Australia") %>% 
  mutate(Pop = Population/1e6) %>% 
  model(
    AAN = ETS(Pop ~ error("A") + trend("A") + season("N") ))

fc <-fit_aus %>%  forecast(h = 10)

#high alpha means that level equation is short-sighted
#low-range beta means the trend gives a large emphasis on past-step trend
report(fit_aus)

#sanity checks
glimpse(fc)
glimpse(augment(fit_aus))
tail(augment(fit_aus))
tail(fc)

```

`model()` calls this `ETS(A,Ad,N)`

This method (Holt's linear trend) can overstate the trend component in forecasting
A recent more popular method is the Damped trend method. It introduces a dampening parameter $\phi$ which is bound by practitioners between $0.8 \leq \phi \leq 0.98$
The method introduces the parameter in all three equations, dampening $b_{t-1}$

```{r}
#"Ad" doesnt let you specify phi less than 0.7
aus_fits <- global_economy %>% 
  filter(Country == "Australia") %>% 
  mutate(Pop = Population/1e6) %>%  
  model(
    ANN = ETS(Pop ~ error("A") + trend("N") + season("N")),
    AAN = ETS(Pop ~ error("A") + trend("A") + season("N")),
    AAdN_90 = ETS(Pop ~ error("A") + trend("Ad", phi = 0.9) + season("N")),
    AAdN_98 = ETS(Pop ~ error("A") + trend("Ad", phi = 0.98) + season("N")),
    AAdN_70 = ETS(Pop ~ error("A") + trend("Ad", phi = 0.8) + season("N")) )

report(aus_fits)

fc <- aus_fits %>% forecast(h=15)

aus_economy <- global_economy %>% 
  filter(Country == "Australia") %>% 
  mutate(Pop = Population/1e6)

fc %>% 
  autoplot(aus_economy)


```

```{r}
www_usage <- as_tsibble(WWWusage)

www_usage %>% 
  model(
    AAdN = ETS(value ~ error("A") + trend("Ad") + season("N")) ) %>% 
  forecast(h=10) %>% 
  autoplot(www_usage) +
  labs(x="Minute", y="Number of users",
       title = "Internet usage per minute")

```

### Methods with seasonality

`model()` calls this `ETS(A,A,A)`

Building up on the Holt trend method, the Holt-Winters additive method adds the last component $s_t$, which, as before, comes with a smoothing parameter (this time $\gamma$). A few notes:
*   Recall quarterly seasons means m = 4 and yearly means m = 12
*   The trend equation is fixed. The level equation uses seasonally adjusted observations
*   The seasonality equation is a weighted average of the current seasonal index with the index for same season last year
*   $s_t = \gamma^*(y_t - l_t) + (1-\gamma^*)s_{t-m}$

`model()` calls this `ETS(M,A,M)`

Similarly, the Holt-Winters multiplicative method multiplies the seasonal component with the trend and error components
*   Forecast at h = (level + h*trend) X seasonal

We can also dampen across both Holt-Winters seasonal methods

```{r}
aus_holidays <- tourism %>%
  filter(Purpose == "Holiday") %>%
  summarise(Trips = sum(Trips)/1e3)
fit <- aus_holidays %>%
  model(
    additive = ETS(Trips ~ error("A") + trend("A") + 
                                                season("A")),
    multiplicative = ETS(Trips ~ error("M") + trend("A") + 
                                                season("M"))
  )

fc <- fit %>% forecast(h = 12) #12 quarters
fc %>%
  autoplot(aus_holidays, level = NULL) +
  labs(title="Australian domestic tourism",
       y="Overnight trips (millions)") +
  guides(colour = guide_legend(title = "Forecast"))
```

